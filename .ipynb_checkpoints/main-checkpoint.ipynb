{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import calculate_append_severity_scores, encode_class_scores\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/sazle/Downloads/violations2.csv')\n",
    "df = encode_class_scores(violation_df=df)\n",
    "df = calculate_append_severity_scores(violation_df=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>violationid</th>\n",
       "      <th>buildingid</th>\n",
       "      <th>registrationid</th>\n",
       "      <th>boroid</th>\n",
       "      <th>boro</th>\n",
       "      <th>housenumber</th>\n",
       "      <th>lowhousenumber</th>\n",
       "      <th>highhousenumber</th>\n",
       "      <th>streetname</th>\n",
       "      <th>streetcode</th>\n",
       "      <th>...</th>\n",
       "      <th>councildistrict</th>\n",
       "      <th>censustract</th>\n",
       "      <th>bin</th>\n",
       "      <th>bbl</th>\n",
       "      <th>nta</th>\n",
       "      <th>certifieddate</th>\n",
       "      <th>newcertifybydate</th>\n",
       "      <th>newcorrectbydate</th>\n",
       "      <th>risk_score</th>\n",
       "      <th>risk_score_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10413107</td>\n",
       "      <td>53116</td>\n",
       "      <td>204446</td>\n",
       "      <td>2</td>\n",
       "      <td>BRONX</td>\n",
       "      <td>3565</td>\n",
       "      <td>3565</td>\n",
       "      <td>3565</td>\n",
       "      <td>BRUCKNER BOULEVARD</td>\n",
       "      <td>15920</td>\n",
       "      <td>...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>26602.0</td>\n",
       "      <td>2046331.0</td>\n",
       "      <td>2.041780e+09</td>\n",
       "      <td>Pelham Bay-Country Club-City Island</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10325067</td>\n",
       "      <td>70433</td>\n",
       "      <td>222588</td>\n",
       "      <td>2</td>\n",
       "      <td>BRONX</td>\n",
       "      <td>1120</td>\n",
       "      <td>1120</td>\n",
       "      <td>1120</td>\n",
       "      <td>EAST 225 STREET</td>\n",
       "      <td>28380</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>386.0</td>\n",
       "      <td>2090246.0</td>\n",
       "      <td>2.049030e+09</td>\n",
       "      <td>Eastchester-Edenwald-Baychester</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10318007</td>\n",
       "      <td>43087</td>\n",
       "      <td>126972</td>\n",
       "      <td>1</td>\n",
       "      <td>MANHATTAN</td>\n",
       "      <td>504</td>\n",
       "      <td>504</td>\n",
       "      <td>506</td>\n",
       "      <td>WEST 167 STREET</td>\n",
       "      <td>37110</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>251.0</td>\n",
       "      <td>1062962.0</td>\n",
       "      <td>1.021230e+09</td>\n",
       "      <td>Washington Heights South</td>\n",
       "      <td>2014-09-25T00:00:00.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10281624</td>\n",
       "      <td>92541</td>\n",
       "      <td>202243</td>\n",
       "      <td>2</td>\n",
       "      <td>BRONX</td>\n",
       "      <td>1023</td>\n",
       "      <td>1023</td>\n",
       "      <td>1023</td>\n",
       "      <td>LONGWOOD AVENUE</td>\n",
       "      <td>46820</td>\n",
       "      <td>...</td>\n",
       "      <td>17.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>2005744.0</td>\n",
       "      <td>2.027210e+09</td>\n",
       "      <td>Hunts Point</td>\n",
       "      <td>2014-07-03T00:00:00.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10291962</td>\n",
       "      <td>150298</td>\n",
       "      <td>360978</td>\n",
       "      <td>3</td>\n",
       "      <td>BROOKLYN</td>\n",
       "      <td>837</td>\n",
       "      <td>837</td>\n",
       "      <td>837</td>\n",
       "      <td>43 STREET</td>\n",
       "      <td>8730</td>\n",
       "      <td>...</td>\n",
       "      <td>38.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>3018366.0</td>\n",
       "      <td>3.009250e+09</td>\n",
       "      <td>Sunset Park East</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   violationid  buildingid  registrationid  boroid       boro housenumber  \\\n",
       "0     10413107       53116          204446       2      BRONX        3565   \n",
       "1     10325067       70433          222588       2      BRONX        1120   \n",
       "2     10318007       43087          126972       1  MANHATTAN         504   \n",
       "3     10281624       92541          202243       2      BRONX        1023   \n",
       "4     10291962      150298          360978       3   BROOKLYN         837   \n",
       "\n",
       "  lowhousenumber highhousenumber          streetname  streetcode  ...  \\\n",
       "0           3565            3565  BRUCKNER BOULEVARD       15920  ...   \n",
       "1           1120            1120     EAST 225 STREET       28380  ...   \n",
       "2            504             506     WEST 167 STREET       37110  ...   \n",
       "3           1023            1023     LONGWOOD AVENUE       46820  ...   \n",
       "4            837             837           43 STREET        8730  ...   \n",
       "\n",
       "   councildistrict censustract        bin           bbl  \\\n",
       "0             13.0     26602.0  2046331.0  2.041780e+09   \n",
       "1             12.0       386.0  2090246.0  2.049030e+09   \n",
       "2             10.0       251.0  1062962.0  1.021230e+09   \n",
       "3             17.0        89.0  2005744.0  2.027210e+09   \n",
       "4             38.0        92.0  3018366.0  3.009250e+09   \n",
       "\n",
       "                                   nta            certifieddate  \\\n",
       "0  Pelham Bay-Country Club-City Island                      NaN   \n",
       "1      Eastchester-Edenwald-Baychester                      NaN   \n",
       "2             Washington Heights South  2014-09-25T00:00:00.000   \n",
       "3                          Hunts Point  2014-07-03T00:00:00.000   \n",
       "4                     Sunset Park East                      NaN   \n",
       "\n",
       "  newcertifybydate newcorrectbydate risk_score risk_score_cat  \n",
       "0              NaN              NaN          1              1  \n",
       "1              NaN              NaN          1              1  \n",
       "2              NaN              NaN          0              0  \n",
       "3              NaN              NaN          1              1  \n",
       "4              NaN              NaN          1              1  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "risk_score\n",
       "1    50507\n",
       "0    25056\n",
       "2    24429\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['risk_score'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"risk_score\", \"risk_score_cat\"])\n",
    "y = df[\"risk_score_cat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['violationid', 'buildingid', 'registrationid', 'boroid', 'boro',\n",
       "       'housenumber', 'lowhousenumber', 'highhousenumber', 'streetname',\n",
       "       'streetcode', 'zip', 'apartment', 'story', 'block', 'lot', 'class',\n",
       "       'inspectiondate', 'approveddate', 'originalcertifybydate',\n",
       "       'originalcorrectbydate', 'ordernumber', 'novid', 'novdescription',\n",
       "       'novissueddate', 'currentstatusid', 'currentstatus',\n",
       "       'currentstatusdate', 'novtype', 'violationstatus', 'rentimpairing',\n",
       "       'latitude', 'longitude', 'communityboard', 'councildistrict',\n",
       "       'censustract', 'bin', 'bbl', 'nta', 'certifieddate', 'newcertifybydate',\n",
       "       'newcorrectbydate'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "violationid\n",
      "violationid\n",
      "10413107    1\n",
      "13928485    1\n",
      "13721122    1\n",
      "13546784    1\n",
      "13910974    1\n",
      "           ..\n",
      "12023463    1\n",
      "11623655    1\n",
      "11647953    1\n",
      "11852527    1\n",
      "16174759    1\n",
      "Name: count, Length: 99992, dtype: int64\n",
      "\n",
      "buildingid\n",
      "buildingid\n",
      "65537     64\n",
      "119285    59\n",
      "344337    57\n",
      "77561     56\n",
      "806375    55\n",
      "          ..\n",
      "705448     1\n",
      "481124     1\n",
      "268783     1\n",
      "649132     1\n",
      "679596     1\n",
      "Name: count, Length: 36294, dtype: int64\n",
      "\n",
      "registrationid\n",
      "registrationid\n",
      "0         3057\n",
      "207907      64\n",
      "201131      59\n",
      "332011      57\n",
      "201115      56\n",
      "          ... \n",
      "207852       1\n",
      "344078       1\n",
      "404847       1\n",
      "214298       1\n",
      "401292       1\n",
      "Name: count, Length: 33770, dtype: int64\n",
      "\n",
      "boroid\n",
      "boroid\n",
      "3    36232\n",
      "2    31981\n",
      "1    20925\n",
      "4     9485\n",
      "5     1369\n",
      "Name: count, dtype: int64\n",
      "\n",
      "boro\n",
      "boro\n",
      "BROOKLYN         36232\n",
      "BRONX            31981\n",
      "MANHATTAN        20925\n",
      "QUEENS            9485\n",
      "STATEN ISLAND     1369\n",
      "Name: count, dtype: int64\n",
      "\n",
      "housenumber\n",
      "housenumber\n",
      "20        239\n",
      "15        231\n",
      "125       216\n",
      "1         203\n",
      "250       200\n",
      "         ... \n",
      "135-24      1\n",
      "111-08      1\n",
      "2154        1\n",
      "3159        1\n",
      "36A         1\n",
      "Name: count, Length: 8182, dtype: int64\n",
      "\n",
      "lowhousenumber\n",
      "lowhousenumber\n",
      "1        245\n",
      "2        212\n",
      "250      210\n",
      "20       198\n",
      "11       174\n",
      "        ... \n",
      "8789       1\n",
      "9512       1\n",
      "617A       1\n",
      "47-42      1\n",
      "36A        1\n",
      "Name: count, Length: 8154, dtype: int64\n",
      "\n",
      "highhousenumber\n",
      "highhousenumber\n",
      "20       239\n",
      "15       227\n",
      "180      191\n",
      "130      188\n",
      "131      181\n",
      "        ... \n",
      "81-35      1\n",
      "42-41      1\n",
      "52-65      1\n",
      "71A        1\n",
      "36A        1\n",
      "Name: count, Length: 8240, dtype: int64\n",
      "\n",
      "streetname\n",
      "streetname\n",
      "GRAND CONCOURSE       1421\n",
      "BROADWAY              1016\n",
      "OCEAN AVENUE           995\n",
      "ST NICHOLAS AVENUE     759\n",
      "WALTON AVENUE          758\n",
      "                      ... \n",
      "TABB PLACE               1\n",
      "BRIGHTON 3 PLACE         1\n",
      "BEACH 48 STREET          1\n",
      "CANEY ROAD               1\n",
      "176 PLACE                1\n",
      "Name: count, Length: 3325, dtype: int64\n",
      "\n",
      "streetcode\n",
      "streetcode\n",
      "36420    1421\n",
      "67530     999\n",
      "13610     951\n",
      "72720     758\n",
      "11710     749\n",
      "         ... \n",
      "44590       1\n",
      "37520       1\n",
      "95400       1\n",
      "15350       1\n",
      "24885       1\n",
      "Name: count, Length: 3319, dtype: int64\n",
      "\n",
      "zip\n",
      "zip\n",
      "11226.0    4076\n",
      "10457.0    2962\n",
      "10458.0    2884\n",
      "10453.0    2839\n",
      "10452.0    2753\n",
      "           ... \n",
      "11001.0       3\n",
      "10129.0       1\n",
      "10280.0       1\n",
      "1901.0        1\n",
      "10069.0       1\n",
      "Name: count, Length: 181, dtype: int64\n",
      "\n",
      "apartment\n",
      "apartment\n",
      "2        2683\n",
      "1        2536\n",
      "3        1465\n",
      "1A       1381\n",
      "1B       1321\n",
      "         ... \n",
      "C37         1\n",
      "BMEN        1\n",
      "S21         1\n",
      "53W         1\n",
      "APT2K       1\n",
      "Name: count, Length: 2903, dtype: int64\n",
      "\n",
      "story\n",
      "story\n",
      "1      24804\n",
      "2      18463\n",
      "3      14684\n",
      "4      11485\n",
      "5       8195\n",
      "       ...  \n",
      "233        1\n",
      "808        1\n",
      "46         1\n",
      "883        1\n",
      "501        1\n",
      "Name: count, Length: 71, dtype: int64\n",
      "\n",
      "block\n",
      "block\n",
      "2136     267\n",
      "1988     239\n",
      "2170     212\n",
      "2215     210\n",
      "4271     203\n",
      "        ... \n",
      "13291      1\n",
      "250        1\n",
      "7936       1\n",
      "2737       1\n",
      "970        1\n",
      "Name: count, Length: 7712, dtype: int64\n",
      "\n",
      "lot\n",
      "lot\n",
      "1      6919\n",
      "6      1818\n",
      "5      1616\n",
      "29     1609\n",
      "40     1598\n",
      "       ... \n",
      "317       1\n",
      "545       1\n",
      "658       1\n",
      "493       1\n",
      "239       1\n",
      "Name: count, Length: 508, dtype: int64\n",
      "\n",
      "class\n",
      "class\n",
      "1    50515\n",
      "0    25056\n",
      "2    24421\n",
      "Name: count, dtype: int64\n",
      "\n",
      "inspectiondate\n",
      "inspectiondate\n",
      "2021-04-02T00:00:00.000    179\n",
      "2021-04-01T00:00:00.000    158\n",
      "2022-02-16T00:00:00.000    150\n",
      "2021-04-20T00:00:00.000    144\n",
      "2022-02-22T00:00:00.000    142\n",
      "                          ... \n",
      "2002-06-11T00:00:00.000      1\n",
      "2009-08-17T00:00:00.000      1\n",
      "2005-10-25T00:00:00.000      1\n",
      "1989-03-15T00:00:00.000      1\n",
      "2010-05-08T00:00:00.000      1\n",
      "Name: count, Length: 5125, dtype: int64\n",
      "\n",
      "approveddate\n",
      "approveddate\n",
      "2021-04-02T00:00:00.000    177\n",
      "2021-04-01T00:00:00.000    160\n",
      "2022-02-16T00:00:00.000    148\n",
      "2022-03-01T00:00:00.000    146\n",
      "2021-04-16T00:00:00.000    145\n",
      "                          ... \n",
      "1988-03-14T00:00:00.000      1\n",
      "2013-11-14T00:00:00.000      1\n",
      "2010-08-09T00:00:00.000      1\n",
      "2011-07-15T00:00:00.000      1\n",
      "2010-05-13T00:00:00.000      1\n",
      "Name: count, Length: 5139, dtype: int64\n",
      "\n",
      "originalcertifybydate\n",
      "originalcertifybydate\n",
      "2021-07-20T00:00:00.000    192\n",
      "2021-07-26T00:00:00.000    153\n",
      "2021-08-03T00:00:00.000    139\n",
      "2021-07-19T00:00:00.000    139\n",
      "2022-06-11T00:00:00.000    133\n",
      "                          ... \n",
      "2020-05-13T00:00:00.000      1\n",
      "2016-02-20T00:00:00.000      1\n",
      "2017-02-18T00:00:00.000      1\n",
      "2014-02-14T00:00:00.000      1\n",
      "2014-01-16T00:00:00.000      1\n",
      "Name: count, Length: 3720, dtype: int64\n",
      "\n",
      "originalcorrectbydate\n",
      "originalcorrectbydate\n",
      "2021-07-06T00:00:00.000    194\n",
      "2021-07-12T00:00:00.000    153\n",
      "2021-07-05T00:00:00.000    144\n",
      "2022-05-28T00:00:00.000    138\n",
      "2021-07-20T00:00:00.000    136\n",
      "                          ... \n",
      "2014-02-02T00:00:00.000      1\n",
      "2017-06-17T00:00:00.000      1\n",
      "2016-11-13T00:00:00.000      1\n",
      "2016-05-01T00:00:00.000      1\n",
      "2014-03-30T00:00:00.000      1\n",
      "Name: count, Length: 3713, dtype: int64\n",
      "\n",
      "ordernumber\n",
      "ordernumber\n",
      "508     10453\n",
      "502      7178\n",
      "501      6050\n",
      "556      5726\n",
      "1507     3998\n",
      "        ...  \n",
      "1032        1\n",
      "835         1\n",
      "630         1\n",
      "771         1\n",
      "225         1\n",
      "Name: count, Length: 264, dtype: int64\n",
      "\n",
      "novid\n",
      "novid\n",
      "5361744    4\n",
      "5466131    4\n",
      "5412645    4\n",
      "8459877    4\n",
      "6565054    4\n",
      "          ..\n",
      "5731010    1\n",
      "5842778    1\n",
      "5783897    1\n",
      "5816450    1\n",
      "8607189    1\n",
      "Name: count, Length: 97416, dtype: int64\n",
      "\n",
      "novdescription\n",
      "novdescription\n",
      "(A) ï¿½ HMC:FILE ANNUAL BEDBUG REPORT IN ACCORDANCE WITH HPD RULE AS DESCRIBED ON THE BACK OF THIS NOTICE OF VIOLATION OR AS DESCRIBED ON HPD?S WEBSITE, WWW.NYC.GOV\\HPD, SEARCH BED BUGS.                                                                       1950\n",
      "(A) ï¿½ HMC:FILE ANNUAL BEDBUG REPORT IN ACCORDANCE WITH HPD RULE AS DESCRIBED ON THE BACK OF THIS NOTICE OF VIOLATION OR AS DESCRIBED ON HPD'S WEBSITE, WWW.NYC.GOV\\HPD, SEARCH BED BUGS.                                                                       1376\n",
      "(A) § HMC:FILE ANNUAL BEDBUG REPORT IN ACCORDANCE WITH HPD RULE AS DESCRIBED ON THE BACK OF THIS NOTICE OF VIOLATION OR AS DESCRIBED ON HPD\u001aS WEBSITE, WWW.NYC.GOV\\HPD, SEARCH BED BUGS.                                                                          672\n",
      "SECTION 26-1103 ADMIN. CODE: POST AND MAINTAIN A PROPER NOTICE ON WALL OF THE ENTRANCE STORY IN ENGLISH AND SPANISH ON THE AVAILABILITY OF THE AGENCY?S HOUSING INFORMATION GUIDE. A SAMPLE NOTICE CAN BE FOUND AT WWW.NYC.GOV/HPD. AT PUBLIC HALL, 1st STORY     441\n",
      "SECTION 27-2104 ADM CODE POST AND MAINTAIN A PROPER SIGN ON WALL OF ENTRANCE STORY SHOWING THE REGISTRATION NUMBER ASSIGNED BY THE DEPARTMENT AND THE ADDRESS OF THE BUILDING. AT PUBLIC HALL, 1st STORY                                                          414\n",
      "                                                                                                                                                                                                                                                                 ... \n",
      "SECTION 27-2013 ADM CODE PAINT WITH LIGHT COLORED PAINT TO THE SATISFACTION OF THIS DEPARTMENT ALL WALLS AND CEILINGS IN THE ENTIRE APARTMENT LOCATED AT APT 1F, 1st STORY                                                                                          1\n",
      "SECTION 27-2005, 2007 ADM CODE REMOVE THE ILLEGAL FASTENING CONSISTING OF A SLIDE BOLT AND HASP INSTALLED ON DOOR CAPABLE OF BEING LOCKED IN THE ENTRANCE LOCATED AT APT 1A, 1st STORY, 2nd APARTMENT FROM SOUTH AT WEST                                            1\n",
      "SECTION 27-2013 ADM CODE PAINT WITH LIGHT COLORED PAINT TO THE SATISFACTION OF THIS DEPARTMENT THE BASEBOARD AT SOUTH WALL IN THE PRIVATE HALLWAY LOCATED AT APT 3, 3rd STORY, 1st APARTMENT FROM NORTH AT EAST                                                     1\n",
      "SECTION 27-2046.1 HMC: REPAIR OR REPLACE THE CARBON MONOXIDE DETECTING DEVICE(S). INOPERATIVE IN THE ENTIRE APARTMENT LOCATED AT APT 2J, 2nd STORY, 1st APARTMENT FROM SOUTH AT WEST                                                                                1\n",
      "§ 27-2005, 2007 ADM CODE REMOVE ALL ENCUMBRANCES CONSISTING OF BICYCLE UNDER NORTH STAIRCASE AT PUBLIC HALL, 1st STORY                                                                                                                                              1\n",
      "Name: count, Length: 85289, dtype: int64\n",
      "\n",
      "novissueddate\n",
      "novissueddate\n",
      "2021-04-02T00:00:00.000    181\n",
      "2022-02-22T00:00:00.000    177\n",
      "2021-04-12T00:00:00.000    158\n",
      "2021-04-01T00:00:00.000    153\n",
      "2021-04-05T00:00:00.000    149\n",
      "                          ... \n",
      "2020-03-27T00:00:00.000      4\n",
      "2020-05-29T00:00:00.000      4\n",
      "2020-05-01T00:00:00.000      4\n",
      "2020-04-24T00:00:00.000      4\n",
      "2020-04-02T00:00:00.000      3\n",
      "Name: count, Length: 2491, dtype: int64\n",
      "\n",
      "currentstatusid\n",
      "currentstatusid\n",
      "19    48179\n",
      "9     30847\n",
      "2     12374\n",
      "22     2521\n",
      "21     2199\n",
      "11     1136\n",
      "4       617\n",
      "28      590\n",
      "20      570\n",
      "47      562\n",
      "7       108\n",
      "23      107\n",
      "8        84\n",
      "6        59\n",
      "10       23\n",
      "3         8\n",
      "50        5\n",
      "24        2\n",
      "49        1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "currentstatus\n",
      "currentstatus\n",
      "VIOLATION CLOSED                            48179\n",
      "VIOLATION DISMISSED                         30847\n",
      "NOV SENT OUT                                12374\n",
      "FIRST NO ACCESS TO RE- INSPECT VIOLATION     2521\n",
      "NOT COMPLIED WITH                            2199\n",
      "VIOLATION WILL BE REINSPECTED                1136\n",
      "NOV CERTIFIED LATE                            617\n",
      "INVALID CERTIFICATION                         590\n",
      "DEFECT LETTER ISSUED                          570\n",
      "NOTICE OF ISSUANCE SENT TO TENANT             562\n",
      "CERTIFICATION POSTPONMENT DENIED              108\n",
      "SECOND NO ACCESS TO RE-INSPECT VIOLATION      107\n",
      "FALSE CERTIFICATION                            84\n",
      "CERTIFICATION POSTPONMENT GRANTED              59\n",
      "CIV14 MAILED                                   23\n",
      "NOV CERTIFIED ON TIME                           8\n",
      "LEAD DOCS SUBMITTED, NOT ACCEPTABLE             5\n",
      "VIOLATION REOPEN                                2\n",
      "LEAD DOCS SUBMITTED, ACCEPTABLE                 1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "currentstatusdate\n",
      "currentstatusdate\n",
      "2023-03-20T00:00:00.000    131\n",
      "2022-02-22T00:00:00.000    125\n",
      "2021-04-19T00:00:00.000    117\n",
      "2015-10-02T00:00:00.000    115\n",
      "2021-04-12T00:00:00.000    104\n",
      "                          ... \n",
      "2014-10-05T00:00:00.000      1\n",
      "2014-08-03T00:00:00.000      1\n",
      "2022-06-12T00:00:00.000      1\n",
      "2019-11-28T00:00:00.000      1\n",
      "2024-02-19T00:00:00.000      1\n",
      "Name: count, Length: 3875, dtype: int64\n",
      "\n",
      "novtype\n",
      "novtype\n",
      "Original    97543\n",
      "Reissued     2449\n",
      "Name: count, dtype: int64\n",
      "\n",
      "violationstatus\n",
      "violationstatus\n",
      "Close    79976\n",
      "Open     20016\n",
      "Name: count, dtype: int64\n",
      "\n",
      "rentimpairing\n",
      "rentimpairing\n",
      "N    92823\n",
      "Y     7169\n",
      "Name: count, dtype: int64\n",
      "\n",
      "latitude\n",
      "latitude\n",
      "40.832584    65\n",
      "40.829399    59\n",
      "40.639709    57\n",
      "40.844174    56\n",
      "40.833578    55\n",
      "             ..\n",
      "40.690941     1\n",
      "40.665681     1\n",
      "40.628692     1\n",
      "40.864213     1\n",
      "40.710824     1\n",
      "Name: count, Length: 33792, dtype: int64\n",
      "\n",
      "longitude\n",
      "longitude\n",
      "-73.869201    64\n",
      "-73.922802    59\n",
      "-73.953068    58\n",
      "-73.891670    56\n",
      "-73.902879    55\n",
      "              ..\n",
      "-73.919085     1\n",
      "-74.085839     1\n",
      "-73.755343     1\n",
      "-74.001012     1\n",
      "-73.853100     1\n",
      "Name: count, Length: 32834, dtype: int64\n",
      "\n",
      "communityboard\n",
      "communityboard\n",
      "12.0    11558\n",
      "4.0      9822\n",
      "9.0      9613\n",
      "5.0      9407\n",
      "7.0      7911\n",
      "3.0      7561\n",
      "8.0      5667\n",
      "1.0      5595\n",
      "14.0     5152\n",
      "11.0     5015\n",
      "10.0     4909\n",
      "6.0      4874\n",
      "17.0     4251\n",
      "2.0      3683\n",
      "16.0     1961\n",
      "13.0     1134\n",
      "15.0      942\n",
      "18.0      847\n",
      "Name: count, dtype: int64\n",
      "\n",
      "councildistrict\n",
      "councildistrict\n",
      "15.0    5966\n",
      "14.0    5671\n",
      "10.0    5075\n",
      "40.0    5048\n",
      "16.0    4915\n",
      "17.0    4472\n",
      "9.0     4341\n",
      "7.0     4197\n",
      "41.0    3899\n",
      "36.0    3741\n",
      "8.0     3524\n",
      "37.0    3254\n",
      "35.0    3086\n",
      "42.0    2939\n",
      "11.0    2697\n",
      "45.0    2615\n",
      "18.0    2538\n",
      "34.0    2356\n",
      "12.0    2170\n",
      "38.0    1721\n",
      "39.0    1431\n",
      "3.0     1401\n",
      "13.0    1385\n",
      "44.0    1331\n",
      "48.0    1293\n",
      "2.0     1223\n",
      "49.0    1157\n",
      "33.0    1155\n",
      "47.0    1140\n",
      "31.0    1113\n",
      "43.0    1084\n",
      "6.0      991\n",
      "1.0      953\n",
      "21.0     940\n",
      "26.0     841\n",
      "5.0      799\n",
      "22.0     742\n",
      "27.0     738\n",
      "25.0     734\n",
      "24.0     681\n",
      "28.0     634\n",
      "29.0     614\n",
      "20.0     587\n",
      "46.0     565\n",
      "4.0      550\n",
      "30.0     517\n",
      "32.0     512\n",
      "23.0     191\n",
      "19.0     163\n",
      "50.0     148\n",
      "51.0      64\n",
      "Name: count, dtype: int64\n",
      "\n",
      "censustract\n",
      "censustract\n",
      "225.0       780\n",
      "245.0       740\n",
      "291.0       687\n",
      "261.0       657\n",
      "253.0       654\n",
      "           ... \n",
      "138501.0      1\n",
      "91601.0       1\n",
      "63302.0       1\n",
      "707.0         1\n",
      "56302.0       1\n",
      "Name: count, Length: 1390, dtype: int64\n",
      "\n",
      "bin\n",
      "bin\n",
      "2025588.0    64\n",
      "2002915.0    59\n",
      "3120374.0    57\n",
      "2009965.0    56\n",
      "2002176.0    55\n",
      "             ..\n",
      "3042693.0     1\n",
      "3101745.0     1\n",
      "3028464.0     1\n",
      "4090139.0     1\n",
      "4077087.0     1\n",
      "Name: count, Length: 36086, dtype: int64\n",
      "\n",
      "bbl\n",
      "bbl\n",
      "3.042719e+09    171\n",
      "4.159260e+09    100\n",
      "2.024310e+09     88\n",
      "3.013020e+09     84\n",
      "5.012720e+09     75\n",
      "               ... \n",
      "3.015050e+09      1\n",
      "1.004760e+09      1\n",
      "3.018640e+09      1\n",
      "3.042660e+09      1\n",
      "4.032200e+09      1\n",
      "Name: count, Length: 34967, dtype: int64\n",
      "\n",
      "nta\n",
      "nta\n",
      "Flatbush                                  2980\n",
      "Crown Heights North                       2653\n",
      "Washington Heights South                  2348\n",
      "Mount Hope                                2321\n",
      "Prospect Lefferts Gardens-Wingate         2110\n",
      "                                          ... \n",
      "Douglaston-Little Neck                       1\n",
      "Bay Terrace-Clearview                        1\n",
      "Tottenville-Charleston                       1\n",
      "Whitestone-Beechhurst                        1\n",
      "Annadale-Huguenot-Prince's Bay-Woodrow       1\n",
      "Name: count, Length: 305, dtype: int64\n",
      "\n",
      "certifieddate\n",
      "certifieddate\n",
      "2015-06-01T00:00:00.000    46\n",
      "2020-01-08T00:00:00.000    45\n",
      "2018-01-16T00:00:00.000    40\n",
      "2019-10-03T00:00:00.000    39\n",
      "2016-10-28T00:00:00.000    39\n",
      "                           ..\n",
      "2004-01-16T00:00:00.000     1\n",
      "2016-10-22T00:00:00.000     1\n",
      "2016-10-02T00:00:00.000     1\n",
      "2016-07-03T00:00:00.000     1\n",
      "2023-07-22T00:00:00.000     1\n",
      "Name: count, Length: 3440, dtype: int64\n",
      "\n",
      "newcertifybydate\n",
      "newcertifybydate\n",
      "2015-09-07T00:00:00.000    4\n",
      "2016-08-22T00:00:00.000    3\n",
      "2022-06-02T00:00:00.000    3\n",
      "2020-05-25T00:00:00.000    3\n",
      "2015-09-12T00:00:00.000    3\n",
      "                          ..\n",
      "2017-05-03T00:00:00.000    1\n",
      "2017-08-21T00:00:00.000    1\n",
      "2017-09-05T00:00:00.000    1\n",
      "2018-01-28T00:00:00.000    1\n",
      "2023-06-02T00:00:00.000    1\n",
      "Name: count, Length: 731, dtype: int64\n",
      "\n",
      "newcorrectbydate\n",
      "newcorrectbydate\n",
      "2015-09-02T00:00:00.000    4\n",
      "2016-08-17T00:00:00.000    3\n",
      "2022-05-28T00:00:00.000    3\n",
      "2020-05-20T00:00:00.000    3\n",
      "2015-09-07T00:00:00.000    3\n",
      "                          ..\n",
      "2017-04-28T00:00:00.000    1\n",
      "2017-08-16T00:00:00.000    1\n",
      "2017-08-31T00:00:00.000    1\n",
      "2018-01-23T00:00:00.000    1\n",
      "2023-05-28T00:00:00.000    1\n",
      "Name: count, Length: 731, dtype: int64\n",
      "\n",
      "risk_score\n",
      "risk_score\n",
      "1    50507\n",
      "0    25056\n",
      "2    24429\n",
      "Name: count, dtype: int64\n",
      "\n",
      "risk_score_cat\n",
      "risk_score_cat\n",
      "1    50507\n",
      "0    25056\n",
      "2    24429\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for column in df.columns:\n",
    "    print(f\"{column}\")\n",
    "    print(df[column].value_counts())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\"violationid\", \"buildingid\", \"registrationid\", \"boroid\", \"housenumber\", \"lowhousenumber\", \"highhousenumber\", \"streetname\",\n",
    "                   \"inspectiondate\", \"approveddate\", \"originalcertifybydate\", \"originalcorrectbydate\",\n",
    "                   \"ordernumber\", \"novid\", \"novdescription\", \"novissueddate\", \"currentstatus\", \"novtype\", \n",
    "                    \"bin\", \"bbl\", \"nta\", \"certifieddate\", \"newcertifybydate\", \"newcorrectbydate\", \"currentstatusdate\", \"class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert low cardinality to other\n",
    "# streetcode, apartment, story,\n",
    "# normalize / bucketize latitude and longtitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>boro</th>\n",
       "      <th>streetcode</th>\n",
       "      <th>zip</th>\n",
       "      <th>apartment</th>\n",
       "      <th>story</th>\n",
       "      <th>block</th>\n",
       "      <th>lot</th>\n",
       "      <th>currentstatusid</th>\n",
       "      <th>violationstatus</th>\n",
       "      <th>rentimpairing</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>communityboard</th>\n",
       "      <th>councildistrict</th>\n",
       "      <th>censustract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BRONX</td>\n",
       "      <td>15920</td>\n",
       "      <td>10461.0</td>\n",
       "      <td>2D</td>\n",
       "      <td>2</td>\n",
       "      <td>4178</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>Close</td>\n",
       "      <td>N</td>\n",
       "      <td>40.847944</td>\n",
       "      <td>-73.827568</td>\n",
       "      <td>10.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>26602.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BRONX</td>\n",
       "      <td>28380</td>\n",
       "      <td>10466.0</td>\n",
       "      <td>2F</td>\n",
       "      <td>2</td>\n",
       "      <td>4903</td>\n",
       "      <td>59</td>\n",
       "      <td>9</td>\n",
       "      <td>Close</td>\n",
       "      <td>N</td>\n",
       "      <td>40.883849</td>\n",
       "      <td>-73.848345</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>386.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MANHATTAN</td>\n",
       "      <td>37110</td>\n",
       "      <td>10032.0</td>\n",
       "      <td>5C</td>\n",
       "      <td>5</td>\n",
       "      <td>2123</td>\n",
       "      <td>59</td>\n",
       "      <td>19</td>\n",
       "      <td>Close</td>\n",
       "      <td>N</td>\n",
       "      <td>40.839429</td>\n",
       "      <td>-73.937687</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>251.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BRONX</td>\n",
       "      <td>46820</td>\n",
       "      <td>10459.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>2721</td>\n",
       "      <td>41</td>\n",
       "      <td>9</td>\n",
       "      <td>Close</td>\n",
       "      <td>N</td>\n",
       "      <td>40.816500</td>\n",
       "      <td>-73.896505</td>\n",
       "      <td>2.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>89.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BROOKLYN</td>\n",
       "      <td>8730</td>\n",
       "      <td>11232.0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>925</td>\n",
       "      <td>64</td>\n",
       "      <td>19</td>\n",
       "      <td>Close</td>\n",
       "      <td>N</td>\n",
       "      <td>40.644506</td>\n",
       "      <td>-73.998732</td>\n",
       "      <td>12.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>92.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        boro  streetcode      zip apartment story  block  lot  \\\n",
       "0      BRONX       15920  10461.0        2D     2   4178    5   \n",
       "1      BRONX       28380  10466.0        2F     2   4903   59   \n",
       "2  MANHATTAN       37110  10032.0        5C     5   2123   59   \n",
       "3      BRONX       46820  10459.0       NaN     5   2721   41   \n",
       "4   BROOKLYN        8730  11232.0         8     3    925   64   \n",
       "\n",
       "   currentstatusid violationstatus rentimpairing   latitude  longitude  \\\n",
       "0               19           Close             N  40.847944 -73.827568   \n",
       "1                9           Close             N  40.883849 -73.848345   \n",
       "2               19           Close             N  40.839429 -73.937687   \n",
       "3                9           Close             N  40.816500 -73.896505   \n",
       "4               19           Close             N  40.644506 -73.998732   \n",
       "\n",
       "   communityboard  councildistrict  censustract  \n",
       "0            10.0             13.0      26602.0  \n",
       "1            12.0             12.0        386.0  \n",
       "2            12.0             10.0        251.0  \n",
       "3             2.0             17.0         89.0  \n",
       "4            12.0             38.0         92.0  "
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X.drop(columns=columns_to_drop)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['communityboard', 'censustract', 'story', 'lot', 'zip', 'streetcode', 'currentstatusid', 'councildistrict', 'apartment', 'block']\n"
     ]
    }
   ],
   "source": [
    "oe_vars = [\"boro\"]\n",
    "ord_vars = [\"violationstatus\", \"rentimpairing\"]\n",
    "scaler_vars = [\"latitude\", \"longitude\"]\n",
    "pass_through_vars = list(set(list(X.columns)).difference(set(oe_vars + ord_vars + scaler_vars)))\n",
    "print(pass_through_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apartment\n",
      "2.0     41939\n",
      "1.0     13659\n",
      "10.0    11356\n",
      "3.0     11071\n",
      "4.0      8307\n",
      "5.0      6135\n",
      "6.0      4245\n",
      "7.0      1487\n",
      "8.0       995\n",
      "9.0       797\n",
      "0.0         1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X[\"apartment\"] = X[\"apartment\"].str.replace(r'\\D', '', regex=True).replace('', 2.0).astype('float').clip(upper=10).fillna(2.0)\n",
    "\n",
    "with pd.option_context('display.max_rows', None):\n",
    "    print(X[\"apartment\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "story\n",
      "1.0     31079\n",
      "2.0     18463\n",
      "3.0     14684\n",
      "4.0     11485\n",
      "5.0      8195\n",
      "1.0      6360\n",
      "6.0      5332\n",
      "0.0      1801\n",
      "10.0     1455\n",
      "7.0       572\n",
      "8.0       317\n",
      "9.0       249\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X[\"story\"] = X[\"story\"].str.replace(r'\\D', '', regex=True).replace('', 1.0).astype('float').clip(upper=10).fillna('1.0')\n",
    "with pd.option_context('display.max_rows', None):\n",
    "    print(X[\"story\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_predict, KFold, cross_validate\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "preprocess = make_column_transformer((OrdinalEncoder(), ord_vars), (StandardScaler(), scaler_vars), (OneHotEncoder(), oe_vars), (\"passthrough\", pass_through_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = X.columns[X.isna().any()].tolist()\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imp = SimpleImputer(strategy=\"most_frequent\")\n",
    "X[cols] = imp.fit_transform(X[cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev, X_test, y_dev, y_test = train_test_split(X, y, test_size=0.2, random_state=24)\n",
    "X_dev_str, X_test_str, y_dev_str, y_test_str = train_test_split(X, y, test_size=0.2, random_state=24, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45572278613930695"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = make_pipeline(preprocess, DecisionTreeClassifier(random_state=42))\n",
    "pipe.fit(X_dev, y_dev)\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5208760438021901"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = make_pipeline(preprocess, RandomForestClassifier(random_state=42))\n",
    "pipe.fit(X_dev, y_dev)\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    \"learning_rate\": [0.1, 0.2, 0.3],\n",
    "    \"n_estimators\": [10, 100, 200],\n",
    "    \"max_depth\": [None, 10, 100]\n",
    "}\n",
    "pipe = make_pipeline(preprocess, GridSearchCV(XGBClassifier(random_state=42), cv=5, param_grid=param_grid))\n",
    "pipe.fit(X_dev, y_dev)\n",
    "print(pipe.named_steps[\"gridsearchcv\"].best_score_)\n",
    "print(pipe.named_steps[\"gridsearchcv\"].best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifer = pipe.named_steps[\"gridsearchcv\"].best_estimator_\n",
    "pipe = make_pipeline(preprocess, classifer)\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "edits starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_scorer.py:548: FutureWarning: The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Scores:\n",
      "Mean AUC Score: 0.6552\n",
      "Average Precision Scores:\n",
      "Mean Average Precision: 0.4831\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import make_scorer, roc_auc_score, average_precision_score\n",
    "DTC_weighted = DecisionTreeClassifier(max_depth=10, random_state=42, class_weight='balanced')\n",
    "pipe_weight = make_pipeline(preprocess, DTC_weighted)\n",
    "scoring = {\n",
    "    'roc_auc': make_scorer(roc_auc_score, multi_class='ovr', needs_proba=True),\n",
    "    'average_precision': 'average_precision'\n",
    "}\n",
    "scores_weighted = cross_validate(\n",
    "    pipe_weight,\n",
    "    X_dev,\n",
    "    y_dev,\n",
    "    cv=5,\n",
    "    scoring=scoring\n",
    ")\n",
    "print(\"AUC Scores:\")\n",
    "print(f\"Mean AUC Score: {scores_weighted['test_roc_auc'].mean():.4f}\")\n",
    "print(\"Average Precision Scores:\")\n",
    "print(f\"Mean Average Precision: {scores_weighted['test_average_precision'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_scorer.py:548: FutureWarning: The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Scores:\n",
      "Mean AUC Score: 0.6676\n",
      "Average Precision Scores:\n",
      "Mean Average Precision: 0.5106\n"
     ]
    }
   ],
   "source": [
    "DTC_weighted = RandomForestClassifier(max_depth=10, random_state=42, class_weight='balanced')\n",
    "pipe_weight = make_pipeline(preprocess, DTC_weighted)\n",
    "scoring = {\n",
    "    'roc_auc': make_scorer(roc_auc_score, multi_class='ovr', needs_proba=True),\n",
    "    'average_precision': 'average_precision'\n",
    "}\n",
    "scores_weighted = cross_validate(\n",
    "    pipe_weight,\n",
    "    X_dev,\n",
    "    y_dev,\n",
    "    cv=5,\n",
    "    scoring=scoring\n",
    ")\n",
    "print(\"AUC Scores:\")\n",
    "print(f\"Mean AUC Score: {scores_weighted['test_roc_auc'].mean():.4f}\")\n",
    "print(\"Average Precision Scores:\")\n",
    "print(f\"Mean Average Precision: {scores_weighted['test_average_precision'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_scorer.py:548: FutureWarning: The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.\n",
      "  warnings.warn(\n",
      "python(18889) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(18890) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(18891) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(18892) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(18893) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(18894) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(18895) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(18896) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: {'decisiontreeclassifier__max_depth': 5, 'decisiontreeclassifier__min_samples_leaf': 10, 'decisiontreeclassifier__min_samples_split': 2}\n",
      "AUC Score: 0.6605926534576162\n",
      "Test Accuracy: 0.5449272463623182\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, roc_auc_score\n",
    "\n",
    "roc_auc_scorer = make_scorer(\n",
    "    roc_auc_score,\n",
    "    multi_class='ovr',\n",
    "    needs_proba=True \n",
    ")\n",
    "\n",
    "DTC = DecisionTreeClassifier(random_state=42, class_weight='balanced')\n",
    "pipe_dtc = make_pipeline(preprocess, DTC)\n",
    "param_grid = {\n",
    "    'decisiontreeclassifier__max_depth': [None, 3, 5, 10],\n",
    "    'decisiontreeclassifier__min_samples_split': [2, 10, 20],\n",
    "    'decisiontreeclassifier__min_samples_leaf': [1, 10, 20],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    pipe_dtc,\n",
    "    param_grid,\n",
    "    scoring=roc_auc_scorer,  # Use custom AUC scorer\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "grid_search.fit(X_dev, y_dev)\n",
    "print(\"Best model:\", grid_search.best_params_)\n",
    "print(\"AUC Score:\", grid_search.best_score_)\n",
    "best_model = grid_search.best_estimator_\n",
    "test_score = best_model.score(X_test, y_test)\n",
    "print(\"Test Accuracy:\", test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 64 candidates, totalling 320 fits\n",
      "Best model: {'xgbclassifier__colsample_bytree': 1.0, 'xgbclassifier__learning_rate': 0.1, 'xgbclassifier__max_depth': None, 'xgbclassifier__n_estimators': 50, 'xgbclassifier__subsample': 1.0}\n",
      "AUC Score: 0.6718458745666925\n",
      "Test accuracy: 0.5698784939246962\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "# XGBoost\n",
    "xgb_model = XGBClassifier(\n",
    "    random_state=42, \n",
    "    eval_metric='mlogloss'  # Use eval_metric for proper evaluation in multiclass\n",
    ")\n",
    "\n",
    "pipe_xgb = make_pipeline(preprocess, xgb_model)\n",
    "param_grid_xgb = {\n",
    "    'xgbclassifier__max_depth': [None, 3, 6, 10],\n",
    "    'xgbclassifier__learning_rate': [0.1, 0.2],\n",
    "    'xgbclassifier__n_estimators': [50, 100],\n",
    "    'xgbclassifier__subsample': [0.8, 1.0],\n",
    "    'xgbclassifier__colsample_bytree': [0.8, 1.0],\n",
    "}\n",
    "\n",
    "grid_search_xgb = GridSearchCV(\n",
    "    pipe_xgb,\n",
    "    param_grid=param_grid_xgb,\n",
    "    scoring='roc_auc_ovr',  \n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "grid_search_xgb.fit(X_dev, y_dev)\n",
    "\n",
    "print(\"Best model:\", grid_search_xgb.best_params_)\n",
    "print(\"AUC Score:\", grid_search_xgb.best_score_)\n",
    "xgb_best_model = grid_search_xgb.best_estimator_\n",
    "xgb_test_score = xgb_best_model.score(X_test, y_test)\n",
    "print(\"Test accuracy:\", xgb_test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the following one is the best one so far, about 58% acc, using stratify splitting and xgb classifier with the following hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "Best model: {'xgbclassifier__colsample_bytree': 0.8, 'xgbclassifier__learning_rate': 0.1, 'xgbclassifier__max_depth': None, 'xgbclassifier__n_estimators': 100, 'xgbclassifier__subsample': 0.8}\n",
      "AUC Score: 0.6738961141095288\n",
      "Test accuracy: 0.5827791389569479\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   1.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   3.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   2.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   2.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   1.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   4.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   6.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  10.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   7.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   3.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   1.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   2.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   5.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   5.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  11.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   5.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   1.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   3.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   4.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   5.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   7.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  14.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   6.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   6.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   6.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  12.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   5.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   2.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   6.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  11.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   4.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   1.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   2.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   3.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   7.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  12.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=  10.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   3.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   3.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   5.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   6.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  12.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=  10.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   3.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   3.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   5.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  12.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=20, xgbclassifier__subsample=0.8; total time=   2.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=20, xgbclassifier__subsample=1.0; total time=   1.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   6.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=20, xgbclassifier__subsample=0.8; total time=   1.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=20, xgbclassifier__subsample=1.0; total time=   1.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   5.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=20, xgbclassifier__subsample=0.8; total time=   3.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   7.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  12.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=  13.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   4.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=20, xgbclassifier__subsample=0.8; total time=   1.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=20, xgbclassifier__subsample=1.0; total time=   1.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   4.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   3.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=20, xgbclassifier__subsample=1.0; total time=   2.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   6.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  11.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=20, xgbclassifier__subsample=0.8; total time=   1.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=20, xgbclassifier__subsample=1.0; total time=   1.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=20, xgbclassifier__subsample=0.8; total time=   1.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=20, xgbclassifier__subsample=1.0; total time=   1.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   4.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   5.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=20, xgbclassifier__subsample=0.8; total time=   3.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   7.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   7.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=  13.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   6.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=20, xgbclassifier__subsample=0.8; total time=   1.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=20, xgbclassifier__subsample=1.0; total time=   2.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   5.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=20, xgbclassifier__subsample=0.8; total time=   3.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   7.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   7.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   9.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   4.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=   7.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=   9.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   7.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=  12.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=  27.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time= 1.7min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time= 1.9min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time= 3.0min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time= 4.8min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time= 2.4min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time= 1.7min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time= 3.2min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time= 4.4min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time= 1.8min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=  25.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time= 1.8min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time= 2.7min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time= 4.0min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   3.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   2.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=   5.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=   5.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   6.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   6.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=  12.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=  50.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time= 2.6min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time= 4.0min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   1.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   1.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=   6.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   8.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   5.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=  14.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=  19.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time= 4.1min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time= 5.9min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time= 8.3min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=11.0min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time= 6.7min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time= 4.3min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time= 5.5min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time= 2.4min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time= 4.9min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time= 6.7min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time= 4.8min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time= 2.1min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time= 2.3min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time= 2.8min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   3.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   1.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   2.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   1.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   4.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   5.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   6.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   9.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   1.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   3.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   1.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   1.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   2.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   6.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  11.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   9.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   5.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   3.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   3.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   5.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   7.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   7.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=  13.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   4.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   1.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   1.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   5.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   4.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   7.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   5.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   5.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=  11.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   4.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   5.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   2.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   5.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   5.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   6.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=  11.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   4.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   3.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   2.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   3.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   4.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   7.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  12.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   4.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   1.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   3.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   5.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   7.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  12.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=  11.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   1.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   3.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   5.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  11.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=20, xgbclassifier__subsample=0.8; total time=   1.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=20, xgbclassifier__subsample=1.0; total time=   1.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   6.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=20, xgbclassifier__subsample=0.8; total time=   2.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=20, xgbclassifier__subsample=1.0; total time=   1.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   5.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=20, xgbclassifier__subsample=0.8; total time=   3.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=20, xgbclassifier__subsample=1.0; total time=   3.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   7.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  12.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=20, xgbclassifier__subsample=0.8; total time=   1.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=20, xgbclassifier__subsample=1.0; total time=   2.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   5.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=20, xgbclassifier__subsample=0.8; total time=   1.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=20, xgbclassifier__subsample=1.0; total time=   1.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   4.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=20, xgbclassifier__subsample=0.8; total time=   2.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=20, xgbclassifier__subsample=1.0; total time=   2.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   7.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=  10.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=20, xgbclassifier__subsample=0.8; total time=   1.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=20, xgbclassifier__subsample=0.8; total time=   2.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=20, xgbclassifier__subsample=1.0; total time=   1.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   4.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=20, xgbclassifier__subsample=0.8; total time=   1.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=20, xgbclassifier__subsample=1.0; total time=   1.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   6.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   5.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=20, xgbclassifier__subsample=1.0; total time=   3.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   6.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  14.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=20, xgbclassifier__subsample=0.8; total time=   1.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=20, xgbclassifier__subsample=1.0; total time=   1.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   4.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   7.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   5.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   5.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=20, xgbclassifier__subsample=1.0; total time=   3.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   6.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  12.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   3.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=   8.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   8.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  13.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=  28.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=  17.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time= 1.9min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time= 3.0min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time= 6.3min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   3.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   3.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=   6.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=   6.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=   7.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   5.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   6.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  14.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=  22.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=  28.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time= 1.9min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time= 3.2min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time= 3.7min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time= 2.4min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=  29.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=  17.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time= 1.5min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time= 2.8min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time= 4.0min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   3.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   3.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=   5.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   4.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   7.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=  13.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=  12.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=  46.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time= 2.9min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time= 3.8min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   1.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   3.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=   6.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   9.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   6.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=  13.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=  18.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time= 3.5min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time= 6.1min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time= 9.4min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=11.9min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time= 5.7min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time= 5.0min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time= 5.5min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time= 3.2min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time= 5.2min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time= 6.0min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   1.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   1.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   1.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   1.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   1.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   1.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   1.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   1.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   1.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   1.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   3.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   3.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   3.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   2.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   3.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   3.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   2.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   2.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   3.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   2.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=   5.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=   5.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=   5.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=   5.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=   5.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=   5.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   4.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   7.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   7.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   6.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   6.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=  14.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=  10.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time= 1.4min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time= 2.1min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time= 2.9min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time= 2.6min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   3.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   3.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   2.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   1.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   4.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   6.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   6.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   8.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   3.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   3.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   1.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   2.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   4.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   5.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   5.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   9.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   5.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   3.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   5.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   7.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  14.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   4.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   4.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   7.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   6.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   5.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=  10.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   6.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   3.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   5.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   5.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   6.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=  10.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   4.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   1.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   1.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   2.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   7.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  13.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   4.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   1.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   3.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   6.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   5.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   7.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=  10.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   5.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   1.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   3.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   4.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   5.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  11.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=20, xgbclassifier__subsample=0.8; total time=   2.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   6.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=20, xgbclassifier__subsample=0.8; total time=   1.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   5.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   5.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=20, xgbclassifier__subsample=1.0; total time=   3.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   7.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  12.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=20, xgbclassifier__subsample=0.8; total time=   2.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=20, xgbclassifier__subsample=1.0; total time=   2.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   4.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=20, xgbclassifier__subsample=0.8; total time=   1.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=20, xgbclassifier__subsample=1.0; total time=   1.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   4.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=20, xgbclassifier__subsample=0.8; total time=   2.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=20, xgbclassifier__subsample=1.0; total time=   2.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   7.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  11.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=20, xgbclassifier__subsample=0.8; total time=   1.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=20, xgbclassifier__subsample=1.0; total time=   1.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   4.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=20, xgbclassifier__subsample=0.8; total time=   1.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=20, xgbclassifier__subsample=1.0; total time=   1.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   6.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=20, xgbclassifier__subsample=0.8; total time=   3.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=20, xgbclassifier__subsample=1.0; total time=   3.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   7.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  13.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=20, xgbclassifier__subsample=0.8; total time=   2.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=20, xgbclassifier__subsample=0.8; total time=   2.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=20, xgbclassifier__subsample=1.0; total time=   1.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   6.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   5.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   5.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=20, xgbclassifier__subsample=1.0; total time=   3.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   6.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  13.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   4.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=   8.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=   9.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   6.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  13.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=  27.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time= 1.7min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time= 2.0min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time= 3.0min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time= 5.1min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   7.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=   9.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=   6.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   5.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   6.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  15.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=  11.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=  34.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time= 2.0min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=  56.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time= 2.2min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time= 5.0min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   5.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=  11.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=  13.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   6.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   5.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  20.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=  30.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time= 1.3min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time= 2.1min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time= 2.4min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time= 2.7min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   1.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   1.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   3.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   3.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=   5.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=   5.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   6.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   6.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=  12.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=  48.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time= 1.4min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time= 1.7min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time= 3.6min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   1.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   3.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   3.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=   8.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   7.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  11.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=  18.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=  16.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time= 4.1min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time= 6.6min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time= 8.4min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   4.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   4.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   3.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   2.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=  10.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=   8.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=   8.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=   8.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=  10.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   4.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   5.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   4.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   8.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   7.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   6.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=  16.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=  16.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=  17.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time= 3.6min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time= 3.8min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time= 6.4min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time= 4.3min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time= 5.7min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time= 2.8min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time= 5.2min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time= 6.7min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time= 4.9min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time= 2.1min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time= 2.9min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time= 2.6min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   3.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   1.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   2.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   1.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   1.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   5.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   5.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  11.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   1.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   3.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   2.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   4.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   5.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=  10.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   5.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   4.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   4.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   6.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   7.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=  13.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   1.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   1.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   6.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   7.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   5.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   6.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  12.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   6.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   2.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   5.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  10.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   4.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   3.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   1.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   3.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   4.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   6.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   7.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=  10.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   4.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   3.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   5.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   7.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  12.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   5.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   3.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   3.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   6.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  12.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=20, xgbclassifier__subsample=0.8; total time=   2.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   7.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=20, xgbclassifier__subsample=1.0; total time=   1.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=20, xgbclassifier__subsample=1.0; total time=   3.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   7.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   7.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=  13.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=20, xgbclassifier__subsample=0.8; total time=   1.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   4.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=20, xgbclassifier__subsample=1.0; total time=   2.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   7.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  11.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=  10.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   3.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=20, xgbclassifier__subsample=1.0; total time=   1.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   6.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=20, xgbclassifier__subsample=0.8; total time=   3.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=20, xgbclassifier__subsample=1.0; total time=   4.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   7.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=  13.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=20, xgbclassifier__subsample=0.8; total time=   2.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=20, xgbclassifier__subsample=1.0; total time=   1.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   4.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   4.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   5.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=20, xgbclassifier__subsample=1.0; total time=   2.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=20, xgbclassifier__subsample=1.0; total time=   3.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   6.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  13.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   5.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   3.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=   8.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   8.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  13.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=  12.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=  23.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time= 1.7min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time= 3.2min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time= 6.3min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time= 2.2min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=  24.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time= 1.6min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time= 3.1min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time= 4.9min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   5.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   4.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   6.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=   9.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=  11.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   5.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  16.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=  15.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=  21.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time= 1.2min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time= 1.6min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time= 2.4min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time= 3.1min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time= 1.1min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=  12.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=  46.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time= 2.0min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time= 1.6min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time= 2.9min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   1.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   3.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   3.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=   7.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   8.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  12.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=  19.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=  18.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time= 4.1min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time= 6.0min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time= 9.0min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   4.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   3.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   3.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   2.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   5.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=   8.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=   8.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=   7.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=   8.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=  10.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   4.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   5.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   4.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   7.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   8.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   7.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=  17.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=  17.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=  16.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=  30.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time= 3.3min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time= 5.1min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time= 5.7min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time= 4.9min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   1.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   1.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   1.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   1.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   3.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   3.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   3.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   3.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   3.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=   6.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=   6.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=   6.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=   5.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   4.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   4.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   4.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   9.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  10.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   8.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=  11.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=  29.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=  20.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=  14.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time= 2.8min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time= 3.6min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time= 5.7min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time= 6.8min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=  15.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=  10.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time= 1.6min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time= 2.1min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time= 2.9min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time= 3.0min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   3.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   2.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   5.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   6.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  10.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   3.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   3.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   2.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   3.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   5.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   5.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  10.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   9.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   5.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   3.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   4.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   8.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  14.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   9.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   1.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   5.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   6.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   6.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  12.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   6.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   2.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   2.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   5.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   6.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  11.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=  10.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   1.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   3.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   6.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  12.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=  10.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   3.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   5.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   6.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  12.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   5.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   1.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   1.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   3.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   4.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   5.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   5.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   9.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=20, xgbclassifier__subsample=1.0; total time=   1.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   6.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=20, xgbclassifier__subsample=0.8; total time=   1.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=20, xgbclassifier__subsample=1.0; total time=   1.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   5.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=20, xgbclassifier__subsample=0.8; total time=   3.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=20, xgbclassifier__subsample=1.0; total time=   3.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   7.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  12.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=20, xgbclassifier__subsample=0.8; total time=   1.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=20, xgbclassifier__subsample=0.8; total time=   2.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=20, xgbclassifier__subsample=1.0; total time=   2.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   5.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=20, xgbclassifier__subsample=0.8; total time=   2.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   6.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   6.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=  11.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   4.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=20, xgbclassifier__subsample=0.8; total time=   1.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=20, xgbclassifier__subsample=1.0; total time=   1.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   6.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=20, xgbclassifier__subsample=0.8; total time=   3.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   7.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   6.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=  13.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=20, xgbclassifier__subsample=1.0; total time=   1.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   4.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   5.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=20, xgbclassifier__subsample=0.8; total time=   2.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   5.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=20, xgbclassifier__subsample=0.8; total time=   3.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   7.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  13.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   5.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=   8.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   8.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  13.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=  12.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=  24.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time= 1.9min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time= 3.2min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time= 6.3min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   4.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   4.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=  11.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=   6.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=   7.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   6.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  11.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=  10.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=  30.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=  25.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time= 1.7min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time= 2.9min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time= 2.3min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time= 3.3min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=  11.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=  14.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   6.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  16.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=  13.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=  23.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time= 1.7min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time= 2.6min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time= 4.0min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time= 1.1min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=  11.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=  50.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time= 3.0min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time= 3.7min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   1.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   3.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=   6.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   9.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   6.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  15.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=  18.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time= 3.6min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time= 6.1min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time= 6.3min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=12.4min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time= 5.5min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time= 5.7min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time= 5.3min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time= 2.7min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time= 5.0min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time= 6.1min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time= 5.5min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time= 1.6min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time= 2.4min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time= 2.8min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time= 2.7min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   3.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   2.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   4.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   6.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  10.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   3.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   2.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   5.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  11.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   5.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   3.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   5.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   8.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  14.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=  10.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   4.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   1.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   1.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   5.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   6.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   6.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  12.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   5.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   5.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   2.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   2.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   5.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   6.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  10.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=  10.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   3.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   1.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   3.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   4.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   7.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   7.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=  10.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   4.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   3.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   3.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   5.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   7.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   7.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=  10.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   5.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   3.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   5.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   5.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  11.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=20, xgbclassifier__subsample=1.0; total time=   1.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   6.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   5.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   5.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=20, xgbclassifier__subsample=0.8; total time=   3.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   7.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  12.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=  12.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   5.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=20, xgbclassifier__subsample=0.8; total time=   2.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   6.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   6.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=  10.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=20, xgbclassifier__subsample=1.0; total time=   1.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   5.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   6.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=20, xgbclassifier__subsample=1.0; total time=   3.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   7.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  13.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=20, xgbclassifier__subsample=0.8; total time=   2.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=20, xgbclassifier__subsample=1.0; total time=   1.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   6.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=20, xgbclassifier__subsample=0.8; total time=   1.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=20, xgbclassifier__subsample=1.0; total time=   2.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   5.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=20, xgbclassifier__subsample=0.8; total time=   3.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   6.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   6.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=  10.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   4.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=   8.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   7.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   7.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=  13.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=  27.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time= 1.7min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time= 3.1min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time= 3.4min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time= 4.0min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   6.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   6.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  14.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=  11.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=  35.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time= 2.0min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time= 3.2min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time= 4.8min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   6.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   3.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=  11.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=  14.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   6.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  10.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=  13.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=  26.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=  18.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time= 1.6min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time= 2.8min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time= 3.9min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   3.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   3.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=   5.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=   5.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   6.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   6.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=  13.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=  50.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time= 1.6min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time= 1.6min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time= 3.3min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=  32.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   1.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   3.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=   6.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=  10.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  12.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=  10.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=  16.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time= 3.5min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time= 4.5min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time= 5.9min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time= 9.3min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time= 2.9min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time= 5.3min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time= 6.5min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time= 4.2min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   3.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   3.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   3.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   3.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   3.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=   6.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=   6.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=   6.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=   5.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   4.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   4.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   4.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   8.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=  10.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   8.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=  26.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=  26.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=  15.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time= 2.8min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time= 3.4min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time= 5.9min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time= 5.9min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=   6.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=   5.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   4.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   7.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   7.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   6.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=  15.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=  12.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=  10.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time= 1.6min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time= 2.5min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time= 2.8min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time= 2.7min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   3.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   1.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   1.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   2.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   1.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   5.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   6.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  10.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   6.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   3.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   1.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   1.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   3.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   5.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   5.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   5.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=  10.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   5.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   3.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   4.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   7.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   7.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=  12.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   4.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   1.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   5.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   7.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   6.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   5.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=  10.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   5.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   2.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   5.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   6.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  11.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   3.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   1.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   1.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   3.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   4.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   5.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   7.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=  10.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   4.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   1.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   4.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   5.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   5.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   6.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  11.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   1.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   3.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   5.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   5.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=  10.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=20, xgbclassifier__subsample=0.8; total time=   2.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   7.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=20, xgbclassifier__subsample=0.8; total time=   2.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   5.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   5.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   6.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   6.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=  12.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=20, xgbclassifier__subsample=1.0; total time=   2.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   5.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=20, xgbclassifier__subsample=0.8; total time=   1.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=20, xgbclassifier__subsample=1.0; total time=   1.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   4.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   3.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=20, xgbclassifier__subsample=1.0; total time=   3.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   6.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  11.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=20, xgbclassifier__subsample=0.8; total time=   1.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=20, xgbclassifier__subsample=1.0; total time=   1.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   5.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   4.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   5.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=20, xgbclassifier__subsample=1.0; total time=   3.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   7.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  13.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=  14.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   5.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=20, xgbclassifier__subsample=0.8; total time=   2.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=20, xgbclassifier__subsample=1.0; total time=   2.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   5.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=20, xgbclassifier__subsample=0.8; total time=   3.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=20, xgbclassifier__subsample=1.0; total time=   3.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   6.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=  12.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=   8.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   8.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   7.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=  12.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=  24.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time= 1.7min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time= 3.1min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time= 3.4min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time= 4.2min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   7.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=  12.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=  11.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=  34.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time= 2.0min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=  54.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time= 2.2min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time= 5.0min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   4.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   3.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   6.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=   9.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=  11.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   5.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  16.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=  14.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=  22.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time= 1.5min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time= 2.5min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time= 3.0min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time= 1.8min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=   5.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   6.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=  13.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=  11.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=  47.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time= 3.0min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time= 3.7min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   1.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   3.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=   7.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=  10.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   6.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=  13.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=  15.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time= 3.4min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time= 4.6min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time= 5.9min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time= 6.9min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   7.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   4.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   8.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   8.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   7.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   7.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=  18.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=  16.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=  16.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time= 3.5min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time= 4.6min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time= 6.2min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time= 5.0min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time= 5.7min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time= 3.6min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time= 5.9min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time= 5.8min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=   6.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=   5.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   4.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   7.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   6.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   6.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=  14.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=  13.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=  10.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time= 1.6min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time= 2.6min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time= 3.0min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time= 2.4min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   1.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   3.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   2.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   1.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   6.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   7.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   7.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   3.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   3.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   3.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   5.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  11.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   4.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   3.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   6.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   8.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  14.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   5.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   4.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   6.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   7.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  12.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   5.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   2.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   5.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   6.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=  10.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   4.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   1.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   3.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   4.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   8.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  12.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   3.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   5.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   6.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   7.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=  10.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   5.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   3.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=3, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   3.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   4.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   5.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   5.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   9.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=20, xgbclassifier__subsample=1.0; total time=   1.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   5.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   5.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=20, xgbclassifier__subsample=1.0; total time=   1.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   5.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=20, xgbclassifier__subsample=0.8; total time=   3.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=20, xgbclassifier__subsample=1.0; total time=   3.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   7.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=  12.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=20, xgbclassifier__subsample=0.8; total time=   1.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=20, xgbclassifier__subsample=1.0; total time=   1.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=20, xgbclassifier__subsample=1.0; total time=   1.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=20, xgbclassifier__subsample=0.8; total time=   2.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   6.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  11.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=  10.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=20, xgbclassifier__subsample=0.8; total time=   1.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   4.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   5.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=20, xgbclassifier__subsample=0.8; total time=   3.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   7.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  12.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.05, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=  14.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   6.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=20, xgbclassifier__subsample=0.8; total time=   1.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=20, xgbclassifier__subsample=1.0; total time=   2.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=6, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   5.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=20, xgbclassifier__subsample=0.8; total time=   3.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=20, xgbclassifier__subsample=1.0; total time=   2.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   7.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  13.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   3.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=   8.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   8.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  13.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=  28.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=  17.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time= 1.9min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time= 3.2min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time= 6.1min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   2.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   2.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   4.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   4.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   6.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=   7.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=   6.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   5.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   5.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  15.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=  23.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=  29.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time= 1.9min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time= 2.9min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time= 2.5min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time= 3.0min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=  10.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=  12.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   5.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   6.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=  18.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=  29.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time= 1.2min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time= 1.7min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time= 2.0min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time= 3.8min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   3.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   3.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=   5.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=   5.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   3.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   3.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   5.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   6.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=  10.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=  51.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time= 2.5min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time= 1.3min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time= 2.8min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=20, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=  28.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   0.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   1.5s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   3.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=   6.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=  10.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  12.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=  10.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=  16.1s\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time= 3.6min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time= 6.1min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time= 5.9min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=11.9min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time= 4.7min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time= 6.6min\n",
      "[CV] END xgbclassifier__colsample_bytree=0.8, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time= 3.8min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=   6.7s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=None, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=   6.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   4.4s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   4.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time=   4.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time=   4.2s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=   9.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time=  11.0s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time=   8.6s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=  26.8s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time=  25.9s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=  15.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=10, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time=  15.3s\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=50, xgbclassifier__subsample=0.8; total time= 2.8min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time= 4.9min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=100, xgbclassifier__subsample=1.0; total time= 6.1min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.1, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time= 5.5min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=50, xgbclassifier__subsample=1.0; total time= 2.0min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=100, xgbclassifier__subsample=0.8; total time= 2.5min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=200, xgbclassifier__subsample=0.8; total time= 2.9min\n",
      "[CV] END xgbclassifier__colsample_bytree=1.0, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=100, xgbclassifier__n_estimators=200, xgbclassifier__subsample=1.0; total time= 2.3min\n"
     ]
    }
   ],
   "source": [
    "xgb_model = XGBClassifier(\n",
    "    random_state=42, \n",
    "    eval_metric='mlogloss' \n",
    ")\n",
    "\n",
    "pipe_xgb = make_pipeline(preprocess, xgb_model)\n",
    "param_grid_xgb = {\n",
    "    'xgbclassifier__max_depth': [None, 10, 100],\n",
    "    'xgbclassifier__learning_rate': [0.1, 0.2],\n",
    "    'xgbclassifier__n_estimators': [50, 100, 200],\n",
    "    'xgbclassifier__subsample': [0.8, 1.0],\n",
    "    'xgbclassifier__colsample_bytree': [0.8, 1.0],\n",
    "}\n",
    "\n",
    "grid_search_xgb = GridSearchCV(\n",
    "    pipe_xgb,\n",
    "    param_grid=param_grid_xgb,\n",
    "    scoring='roc_auc_ovr',  \n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "grid_search_xgb.fit(X_dev_str, y_dev_str)\n",
    "\n",
    "print(\"Best model:\", grid_search_xgb.best_params_)\n",
    "print(\"AUC Score:\", grid_search_xgb.best_score_)\n",
    "xgb_best_model = grid_search_xgb.best_estimator_\n",
    "xgb_test_score = xgb_best_model.score(X_test, y_test)\n",
    "print(\"Test accuracy:\", xgb_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007046 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=  12.2s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012485 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  23.2s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008103 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  22.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009782 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  43.9s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005730 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  56.2s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004521 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  46.0s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005505 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 2.3min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003125 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  17.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007284 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  31.9s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003821 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  50.0s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006449 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time= 1.0min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004649 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=   6.6s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002869 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=   7.7s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006834 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  17.0s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006729 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  28.6s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004273 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  31.1s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009191 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  34.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006784 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  42.9s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004918 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time=  57.3s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005431 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=   9.9s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003962 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=  12.0s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003801 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  19.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007427 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  12.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004438 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  28.9s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004712 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  21.3s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011989 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  33.1s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003550 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time=  57.8s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006435 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time= 1.0min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005744 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=  11.0s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003982 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  13.9s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004835 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  23.1s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006603 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  30.6s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007167 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  32.7s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008815 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time=  52.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004925 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time=  53.9s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004510 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=   9.5s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010238 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  16.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004478 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  13.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004420 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  26.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004866 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  20.0s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007462 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  27.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015982 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time=  60.0s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006120 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time= 1.3min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004573 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=  18.6s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011336 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  23.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010747 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  40.0s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006117 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  22.3s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008090 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  44.3s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005030 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  37.6s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004233 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time=  56.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003710 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=   9.2s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015325 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=   8.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003406 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  13.9s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009290 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  13.1s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004699 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  25.2s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006914 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  18.0s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012674 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  18.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004693 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  28.2s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007336 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  34.3s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003575 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time=  52.0s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017837 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=  10.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007869 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=  10.5s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009070 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  16.2s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009230 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  15.0s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010441 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  31.3s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009108 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  20.3s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005051 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time=  55.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003961 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  38.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006208 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 1.3min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.054907 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=  11.9s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004710 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  35.2s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008834 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  23.6s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006822 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time= 1.3min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004375 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time= 1.2min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003436 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=  10.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003934 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=   9.3s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008236 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  16.4s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009758 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  18.1s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005213 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  24.6s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004822 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  18.9s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006972 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  27.7s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008448 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time=  54.9s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009993 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 1.7min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006637 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  26.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007409 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  21.6s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003872 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time=  54.2s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014314 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time= 1.1min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004337 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=   7.6s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004721 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=   9.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006776 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  13.2s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006766 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  14.2s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004357 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  24.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003780 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  16.7s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005485 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  17.9s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003465 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time=  51.7s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016253 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  20.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006363 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  35.6s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007545 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  51.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027814 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  57.3s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004714 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time= 1.2min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002702 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=  11.2s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008015 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=  13.9s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008932 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  12.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008158 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  13.2s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004271 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  33.7s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007738 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  26.9s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004493 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  28.3s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004501 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time= 1.2min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005208 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 1.7min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007051 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  29.3s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006207 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  20.1s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004727 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  29.3s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005110 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  41.9s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005442 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time=  57.9s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004394 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 2.2min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007416 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time=  58.1s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005620 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time=  55.0s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015788 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=  11.2s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004492 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=   9.1s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004833 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  16.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004717 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  11.6s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005329 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  24.1s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018150 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  21.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003686 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  33.1s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005032 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time=  52.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003986 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time=  54.3s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006253 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=   9.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018337 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=   9.0s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004750 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  14.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004673 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  27.9s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003536 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  29.9s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003580 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  28.9s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004371 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  34.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005931 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time= 1.2min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005732 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 2.7min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008821 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time= 1.1min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006359 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  36.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005567 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 1.8min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009590 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  19.9s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004926 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  27.3s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009388 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  34.7s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004288 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time=  52.0s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004674 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 1.8min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012290 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time=  55.7s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011483 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  42.7s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005256 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 1.3min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011223 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=  11.6s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036007 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  40.1s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009934 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  34.1s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015000 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time= 1.4min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009003 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 2.1min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003499 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  17.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005940 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  28.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002862 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time=  54.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008547 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time=  53.6s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003551 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=  10.0s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006728 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=   9.0s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014090 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  13.9s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004058 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  26.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007132 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  30.7s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006768 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time=  56.0s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004665 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  36.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005781 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 1.8min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015641 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  16.6s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005327 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  26.3s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006245 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time=  52.7s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006265 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 1.6min\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007177 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  26.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007405 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  23.3s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005979 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  29.1s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009557 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  31.9s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006099 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 1.5min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003074 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  17.6s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002910 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  27.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006701 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time=  51.3s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004964 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time=  51.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004642 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=   8.7s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003521 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  13.1s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003780 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  23.7s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003721 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  25.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006748 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005091 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=  12.0s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010260 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  41.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006140 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  42.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006035 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  58.2s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005499 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  43.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004639 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 2.3min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005027 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  18.6s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006598 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  28.2s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005933 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  52.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003893 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time= 1.1min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002545 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 1.7min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007017 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time=  58.2s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006179 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time= 1.2min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003485 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=  11.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003734 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=  11.0s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009103 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  16.1s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003715 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  27.3s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008914 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  29.7s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010526 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  23.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004104 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time=  57.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007278 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time=  55.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004912 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 2.0min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008751 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time=  55.7s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006925 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time=  51.9s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009068 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 1.7min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005128 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  28.6s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008811 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  34.9s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008647 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time= 1.2min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005964 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 2.6min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005983 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  39.1s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005220 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  37.2s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006598 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  37.3s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008985 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 1.8min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010420 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  29.1s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008608 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time=  51.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005358 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time=  51.9s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005101 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=  10.3s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007388 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  15.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004210 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  14.7s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005805 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  29.3s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005405 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  19.6s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007837 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  34.1s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005262 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time=  59.6s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004623 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time=  55.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006145 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  19.2s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006223 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  21.0s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008087 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  23.6s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007550 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time= 1.4min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008994 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time= 1.2min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007741 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=  10.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006987 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=   9.1s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010650 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  16.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005514 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  14.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004670 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  22.6s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003726 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  26.6s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003955 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  28.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009365 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  35.0s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008850 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time=  55.0s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006429 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 1.7min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.050080 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  29.3s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011466 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  42.2s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011593 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time= 1.1min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004792 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=   9.5s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009390 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=   8.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003017 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  13.6s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006850 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  26.7s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011674 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  30.1s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006000 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  27.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004827 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  30.6s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010818 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  32.3s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007681 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 1.8min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004614 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  23.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003077 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time=  45.7s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007740 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 1.5min\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007311 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  31.1s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008723 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  17.9s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006793 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time=  49.7s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010619 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time=  51.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004060 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=   8.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008602 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=   9.2s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007278 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  11.7s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004313 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  12.6s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006280 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  25.6s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004941 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  16.0s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003501 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023006 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  20.9s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006509 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  41.4s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007658 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time= 1.7min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005633 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time= 1.2min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005521 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 2.2min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006543 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time= 1.2min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003400 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time= 1.1min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004815 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 1.6min\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010221 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time= 1.2min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007702 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time=  57.7s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003105 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 2.2min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005363 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time=  57.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003798 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time=  54.9s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004748 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=   9.1s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007301 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=   9.1s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004902 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  16.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004156 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  11.7s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007310 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  24.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005056 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  20.9s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013956 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  24.0s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006639 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time= 1.0min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007510 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time=  54.0s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003999 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=   8.6s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009425 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=  10.1s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006464 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  14.0s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005726 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  14.6s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003418 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  28.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003312 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  18.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016193 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  28.9s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005600 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  34.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004174 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time= 1.2min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005825 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=  11.9s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005944 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=  14.6s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009322 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  36.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019902 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  15.9s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002670 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  37.7s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007616 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  24.1s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004335 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  25.1s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003684 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time= 1.1min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007955 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time=  54.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005942 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=   9.9s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007392 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=   8.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005549 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  13.5s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005259 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  14.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006572 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  25.9s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004871 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  20.3s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004465 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  28.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002787 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time=  53.0s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005871 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 1.7min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005443 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  31.2s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004047 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  24.0s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010286 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time= 1.0min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005031 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time= 1.0min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023551 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  19.5s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007063 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  39.3s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004314 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time= 1.3min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003360 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  48.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003548 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 2.0min\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006362 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  21.1s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005455 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  29.2s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007124 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  35.1s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007315 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time=  55.9s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002863 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=   9.9s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006096 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=   8.3s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010261 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  14.2s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007436 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  14.2s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005466 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  28.0s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005632 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  17.7s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006207 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  29.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004672 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time=  57.6s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004227 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 1.8min\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007333 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  30.0s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010740 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  27.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002882 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  30.9s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004249 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time=  48.0s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004686 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 1.6min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004481 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time=  44.3s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004127 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  32.0s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006273 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 1.5min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007293 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  17.3s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008377 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  30.7s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004432 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  30.9s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005648 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END decisiontreeclassifier__max_depth=None, decisiontreeclassifier__min_samples_leaf=1, decisiontreeclassifier__min_samples_split=2; total time=   3.0s\n",
      "[CV] END decisiontreeclassifier__max_depth=None, decisiontreeclassifier__min_samples_leaf=1, decisiontreeclassifier__min_samples_split=20; total time=   1.8s\n",
      "[CV] END decisiontreeclassifier__max_depth=None, decisiontreeclassifier__min_samples_leaf=10, decisiontreeclassifier__min_samples_split=10; total time=   1.6s\n",
      "[CV] END decisiontreeclassifier__max_depth=None, decisiontreeclassifier__min_samples_leaf=10, decisiontreeclassifier__min_samples_split=20; total time=   2.3s\n",
      "[CV] END decisiontreeclassifier__max_depth=None, decisiontreeclassifier__min_samples_leaf=20, decisiontreeclassifier__min_samples_split=10; total time=   1.8s\n",
      "[CV] END decisiontreeclassifier__max_depth=3, decisiontreeclassifier__min_samples_leaf=1, decisiontreeclassifier__min_samples_split=2; total time=   1.1s\n",
      "[CV] END decisiontreeclassifier__max_depth=3, decisiontreeclassifier__min_samples_leaf=1, decisiontreeclassifier__min_samples_split=10; total time=   0.5s\n",
      "[CV] END decisiontreeclassifier__max_depth=3, decisiontreeclassifier__min_samples_leaf=1, decisiontreeclassifier__min_samples_split=20; total time=   0.5s\n",
      "[CV] END decisiontreeclassifier__max_depth=3, decisiontreeclassifier__min_samples_leaf=10, decisiontreeclassifier__min_samples_split=2; total time=   0.5s\n",
      "[CV] END decisiontreeclassifier__max_depth=3, decisiontreeclassifier__min_samples_leaf=10, decisiontreeclassifier__min_samples_split=10; total time=   0.7s\n",
      "[CV] END decisiontreeclassifier__max_depth=3, decisiontreeclassifier__min_samples_leaf=20, decisiontreeclassifier__min_samples_split=2; total time=   0.5s\n",
      "[CV] END decisiontreeclassifier__max_depth=3, decisiontreeclassifier__min_samples_leaf=20, decisiontreeclassifier__min_samples_split=10; total time=   0.5s\n",
      "[CV] END decisiontreeclassifier__max_depth=5, decisiontreeclassifier__min_samples_leaf=1, decisiontreeclassifier__min_samples_split=2; total time=   0.8s\n",
      "[CV] END decisiontreeclassifier__max_depth=5, decisiontreeclassifier__min_samples_leaf=1, decisiontreeclassifier__min_samples_split=20; total time=   0.7s\n",
      "[CV] END decisiontreeclassifier__max_depth=5, decisiontreeclassifier__min_samples_leaf=10, decisiontreeclassifier__min_samples_split=10; total time=   0.7s\n",
      "[CV] END decisiontreeclassifier__max_depth=5, decisiontreeclassifier__min_samples_leaf=10, decisiontreeclassifier__min_samples_split=20; total time=   0.7s\n",
      "[CV] END decisiontreeclassifier__max_depth=5, decisiontreeclassifier__min_samples_leaf=20, decisiontreeclassifier__min_samples_split=2; total time=   0.8s\n",
      "[CV] END decisiontreeclassifier__max_depth=5, decisiontreeclassifier__min_samples_leaf=20, decisiontreeclassifier__min_samples_split=20; total time=   0.7s\n",
      "[CV] END decisiontreeclassifier__max_depth=10, decisiontreeclassifier__min_samples_leaf=1, decisiontreeclassifier__min_samples_split=2; total time=   0.9s\n",
      "[CV] END decisiontreeclassifier__max_depth=10, decisiontreeclassifier__min_samples_leaf=1, decisiontreeclassifier__min_samples_split=10; total time=   1.9s\n",
      "[CV] END decisiontreeclassifier__max_depth=10, decisiontreeclassifier__min_samples_leaf=10, decisiontreeclassifier__min_samples_split=2; total time=   2.0s\n",
      "[CV] END decisiontreeclassifier__max_depth=10, decisiontreeclassifier__min_samples_leaf=10, decisiontreeclassifier__min_samples_split=10; total time=   1.3s\n",
      "[CV] END decisiontreeclassifier__max_depth=10, decisiontreeclassifier__min_samples_leaf=20, decisiontreeclassifier__min_samples_split=2; total time=   1.7s\n",
      "[CV] END decisiontreeclassifier__max_depth=10, decisiontreeclassifier__min_samples_leaf=20, decisiontreeclassifier__min_samples_split=10; total time=   1.9s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007804 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=  12.6s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009436 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  21.1s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008861 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  21.6s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013888 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  41.2s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010111 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time= 1.4min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004133 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 2.3min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004149 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  32.3s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005705 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  28.2s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003173 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  49.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004883 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  39.9s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027553 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 1.6min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004103 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  19.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006846 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  31.1s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006085 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time= 1.1min\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009661 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 2.2min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004027 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  22.1s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005285 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  32.1s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003196 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time=  58.2s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007839 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 1.8min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002903 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  31.0s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003834 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  32.3s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004516 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  34.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004745 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  35.6s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004540 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 1.6min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003951 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  19.0s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007022 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  33.3s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006957 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time= 1.1min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009545 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 2.6min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003731 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  37.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004242 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  39.1s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004604 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time= 1.0min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006792 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 1.8min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005836 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  27.2s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004719 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  28.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006927 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  33.6s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007265 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  35.9s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004338 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 1.7min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004067 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  18.7s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006262 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  31.3s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006119 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  35.9s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010042 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time= 1.1min\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006812 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time=  56.1s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035091 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=  11.7s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004133 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  35.6s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008288 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  41.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003410 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  49.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004709 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time= 1.2min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005125 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 1.9min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006079 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time=  53.1s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010904 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  36.6s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003569 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 1.7min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008346 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  17.7s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004541 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  29.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004504 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time=  57.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005828 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time=  54.1s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003998 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=   9.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006691 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  13.1s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005942 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  14.3s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006814 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  24.6s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005213 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  16.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003798 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  27.2s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004228 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time=  51.7s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006241 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time=  49.1s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004571 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=   7.2s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003289 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  13.3s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004513 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  12.0s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006875 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  23.7s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004337 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  15.2s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004432 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  25.1s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005777 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time=  46.0s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006853 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time=  46.2s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006514 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=   9.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007151 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  12.1s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004214 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  12.3s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006110 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  27.9s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005477 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  17.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004208 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  27.2s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005061 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time=  51.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003021 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 1.7min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003921 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012952 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=  12.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008273 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  37.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010925 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  22.0s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009647 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time= 1.6min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005195 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time= 1.2min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003134 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=  10.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007634 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=  10.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008417 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  12.9s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007732 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  13.6s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004370 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  33.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013620 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  23.0s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006557 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  27.6s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003707 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time= 1.2min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005331 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time= 1.0min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003942 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=   6.7s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007397 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=   7.6s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005284 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  14.1s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007427 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  17.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007231 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  29.3s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003568 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  20.7s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006568 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time=  59.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007067 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  43.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006136 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 2.0min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006062 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  22.2s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012651 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  31.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007768 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  34.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006220 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  36.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005699 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 1.9min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007401 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  21.6s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004747 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  35.9s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005633 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  36.3s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009696 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 1.6min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004026 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  28.3s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006077 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  18.7s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004005 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time=  56.0s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006394 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  48.6s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012263 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 2.7min\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012619 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  30.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003248 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time= 1.1min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005682 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time=  56.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007231 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 1.9min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019727 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time=  51.7s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005804 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time=  52.0s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007385 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 1.8min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015257 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  30.2s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004157 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  34.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006888 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time=  60.0s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011829 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time=  55.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015642 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=  11.7s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006326 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  18.1s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009323 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  22.7s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007759 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  33.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003561 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time= 1.3min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003794 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time= 1.1min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009584 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=  10.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005655 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  13.3s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009635 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  23.9s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002722 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  25.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011339 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  17.3s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004996 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time=  53.3s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004758 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  37.0s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005777 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 1.9min\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004969 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  24.3s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004059 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time=  56.9s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002913 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time=  55.3s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004394 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 1.9min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007066 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time=  50.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004548 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time=  47.9s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004092 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=   9.1s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002826 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=   7.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003124 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  13.1s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005258 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  22.2s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011413 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  21.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003859 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  16.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006766 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  24.9s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003040 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  30.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003348 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time=  46.1s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008408 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=   9.1s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003563 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=   9.6s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003784 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  13.1s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003225 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  12.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005556 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  27.2s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005849 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  23.0s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004605 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time=  48.9s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009505 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  37.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004226 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009310 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  21.0s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006324 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  21.6s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014218 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  27.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027592 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time= 1.6min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005441 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time= 1.2min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004156 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 2.2min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008451 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time= 1.2min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005735 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time= 1.1min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006192 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=   7.0s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002824 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  13.0s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006819 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  14.0s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009419 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  28.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003861 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  19.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007703 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  31.3s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004785 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time= 1.1min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004262 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time=  57.6s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009128 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=  11.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003112 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  16.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004148 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  13.1s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006307 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  28.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003052 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  35.3s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004223 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  31.3s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004745 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  34.3s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004584 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time=  56.1s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004443 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 2.0min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005340 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time=  55.3s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006785 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  34.4s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007301 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 1.8min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003457 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  20.3s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005845 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time=  56.0s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008612 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time= 1.2min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005236 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=  12.1s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006514 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=  14.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006697 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  35.8s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002916 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  12.1s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005194 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  35.2s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004647 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  39.0s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012337 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  39.7s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005860 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  37.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004518 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time=  56.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005238 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 1.9min\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005988 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time=  58.1s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008221 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time=  52.1s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007828 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=  10.8s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006552 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=  12.1s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009508 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  16.3s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007993 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  29.3s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004144 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  32.3s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010171 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  33.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004360 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  34.7s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005927 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time= 1.0min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009327 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=  11.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027275 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  17.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007145 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  22.5s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008317 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  40.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012218 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  49.6s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008829 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time= 1.2min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004965 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 1.9min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015132 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time=  51.9s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002790 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time=  55.9s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003716 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 1.7min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008611 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  29.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003732 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  32.9s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005030 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time=  57.2s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004376 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 1.8min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008277 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time=  51.1s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012538 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  32.6s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003778 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 1.6min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006510 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  16.6s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008744 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  28.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004610 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  30.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006365 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time=  46.2s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007055 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=   7.7s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005423 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=   9.7s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003091 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  13.2s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004295 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  24.9s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008216 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  29.1s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006946 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  27.0s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004211 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  30.9s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003714 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time=  51.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004035 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 1.7min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005725 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  25.2s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006648 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  30.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006414 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time=  49.1s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005993 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=   8.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005609 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=   7.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005591 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  11.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003976 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  13.0s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009259 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008633 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=  12.1s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008130 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  37.6s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006817 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  37.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007018 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time= 1.5min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004868 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time= 1.3min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003552 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=  11.0s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009423 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  16.7s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005163 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  24.9s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004311 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  37.2s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007181 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  22.0s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004505 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time= 1.2min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005400 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  39.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006355 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 1.6min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003371 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  19.6s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006843 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  31.1s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012267 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  36.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004614 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  43.6s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004403 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 2.0min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007515 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  21.6s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004364 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  32.9s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004518 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  34.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012632 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  36.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006028 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 1.9min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011842 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  22.3s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008208 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  32.9s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006561 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  33.6s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004797 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time=  53.7s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005820 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 1.7min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009270 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time=  55.7s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003937 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  49.1s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.042351 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 2.5min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022078 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  22.9s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012352 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  39.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008348 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time= 1.0min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005152 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=-1, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time=  56.7s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004186 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=   9.7s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005235 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  12.2s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007120 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  24.9s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005776 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  27.0s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005540 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  18.2s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006072 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time=  50.3s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005938 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  35.3s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008292 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 1.7min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005677 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  18.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004593 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  31.1s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019294 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time= 1.1min\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008156 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 1.5min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010611 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  19.3s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012091 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  35.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003587 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  33.3s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004572 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  49.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006281 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  48.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004405 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 2.0min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006459 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  17.1s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004938 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  29.1s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004809 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  34.9s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008040 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time= 1.1min\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005513 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=31; total time=  11.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006055 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  14.2s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004000 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=50; total time=  14.2s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005939 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=50, lgbmclassifier__num_leaves=100; total time=  28.0s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006237 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  17.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015552 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  29.9s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005089 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  32.6s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003575 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  38.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004691 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.01, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 1.8min\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006857 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  18.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006569 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  27.9s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002578 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  30.7s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005631 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time=  47.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004492 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 1.7min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004392 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time=  43.2s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006615 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=50; total time=  47.0s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005581 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=10, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 1.6min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004288 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=100; total time=  51.3s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007672 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  33.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012037 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.1, lgbmclassifier__max_depth=20, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=100; total time= 1.6min\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004012 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=31; total time=  15.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004636 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=100, lgbmclassifier__num_leaves=50; total time=  27.1s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004953 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1808\n",
      "[LightGBM] [Info] Number of data points in the train set: 63994, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  29.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008099 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1809\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[CV] END lgbmclassifier__learning_rate=0.2, lgbmclassifier__max_depth=None, lgbmclassifier__n_estimators=200, lgbmclassifier__num_leaves=31; total time=  32.1s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001169 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1810\n",
      "[LightGBM] [Info] Number of data points in the train set: 79993, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "Best Parameters: {'lgbmclassifier__learning_rate': 0.1, 'lgbmclassifier__max_depth': None, 'lgbmclassifier__n_estimators': 50, 'lgbmclassifier__num_leaves': 31}\n",
      "Best AUC Score: 0.6720108672709106\n",
      "LightGBM Test Set Accuracy: 0.5381769088454422\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "lgb_model = LGBMClassifier(random_state=42, class_weight='balanced')\n",
    "pipe_lgb = make_pipeline(preprocess, lgb_model)\n",
    "param_grid_lgb = {\n",
    "    'lgbmclassifier__num_leaves': [31, 50, 100],\n",
    "    'lgbmclassifier__max_depth': [None, 10, 20],\n",
    "    'lgbmclassifier__learning_rate': [0.01, 0.1, 0.2],\n",
    "    'lgbmclassifier__n_estimators': [50, 100, 200],\n",
    "}\n",
    "\n",
    "grid_search_lgb = GridSearchCV(\n",
    "    pipe_lgb,\n",
    "    param_grid=param_grid_lgb,\n",
    "    scoring=roc_auc_scorer,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "grid_search_lgb.fit(X_dev, y_dev)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search_lgb.best_params_)\n",
    "print(\"Best AUC Score:\", grid_search_lgb.best_score_)\n",
    "\n",
    "best_lgb_model = grid_search_lgb.best_estimator_\n",
    "test_score = best_lgb_model.score(X_test, y_test)\n",
    "print(\"LightGBM Test Set Accuracy:\", test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
